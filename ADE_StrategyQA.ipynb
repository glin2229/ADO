{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-8mkhYELyeNS",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "!pip install wheel setuptools pip --upgrade\n",
        "!pip install --upgrade openai"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import json\n",
        "import random\n",
        "from openai import OpenAI\n",
        "import time\n",
        "import numpy as np\n",
        "import gzip\n",
        "import re\n",
        "\n",
        "API_KEY = ''\n",
        "client = OpenAI(api_key = API_KEY)\n",
        "model_id = 'gpt-3.5-turbo-0125' #gpt-3.5-turbo-0125, gpt-4-turbo-2024-04-09\n",
        "\n",
        "# Replace 'path_to_file.json' with the path to your JSON file\n",
        "file_path = 'BB_strategyQA.json'\n",
        "\n",
        "# Open the file and read the JSON data\n",
        "with open(file_path, 'r') as file:\n",
        "    data = json.load(file)\n",
        "\n",
        "# Now 'data' is a Python dictionary containing the data from the JSON file\n",
        "print(len(data['examples']))\n",
        "\n",
        "no_count = 0 # 1219\n",
        "for i in range(len(data['examples'])):\n",
        "    print(data['examples'][i]['input'])\n",
        "    print(data['examples'][i]['target_scores']['Yes']) # if 0 --> No\n",
        "\n",
        "    if data['examples'][i]['target_scores']['Yes'] == 0:\n",
        "        no_count += 1\n",
        "    print()\n"
      ],
      "metadata": {
        "id": "FEp9ruAr-2SZ",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Q3zRYBg-MJ5-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "APE_prompt = f\"\"\"\n",
        "You are given a boolean question statement that requires multi-step implicit reasoning in order to answer correctly. Your task is to propose a creative,\n",
        "detailed, and step-by-step algorithm to enrich and then reformulate this question statement. The goal is of the algorithm\n",
        "is to perform a thorough data engineering and reformulation on the statement, so that it is easier for a LLM to generate the answer of the question.\n",
        "Below are some example question statements as references:\n",
        "\n",
        "Examples:\n",
        "- {data['examples'][0]['input'] }\n",
        "- {data['examples'][1]['input'] }\n",
        "- {data['examples'][2]['input'] }\n",
        "\n",
        "For each step of the algorithm, number and then start it on a new line; you must start each step with '###------- Step 1: ' The proposed algorithm will later be submitted to a LLM for execution.\n",
        "Important: Do NOT answer the question statement! Do NOT refer to any external database; Do NOT perform vector generations. ONLY propose things that a LLM can do on its own!\n",
        "\"\"\"\n",
        "\n",
        "prompts = []\n",
        "prompt_performances = {}\n",
        "for i in range(1):\n",
        "\n",
        "    completion = client.chat.completions.create(\n",
        "        model = model_id, temperature = 1.0,\n",
        "\n",
        "        messages=[{\"role\": \"system\", \"content\": \"Please come up with a step-by-step, very specific and detailed, clear, and novel algorithm for reformulating and enriching the question statement. Return the steps ONLY!!! NO more than 3 steps. You MUST NOT directly include answer the question!\"},\n",
        "                    {\"role\": \"user\", \"content\": APE_prompt}],\n",
        "        timeout = 1200)\n",
        "\n",
        "    candidate_prompt = completion.choices[0].message.content\n",
        "    prompts.append(candidate_prompt)\n",
        "    print(candidate_prompt)\n",
        "    print()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F_4SWc5lMKDy",
        "outputId": "c5ff6234-d21d-413b-8c10-37b9e29a47e3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "###------- Step 1: Identify the keywords and entities in the question statement.\n",
            "###------- Step 2: Break down the question into sub-questions or statements to clarify the context and relationships between the entities.\n",
            "###------- Step 3: Identify any implied information or assumptions that would be necessary to answer the question accurately.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "\n",
        "# Regular expression to split the text into individual algorithms\n",
        "algorithm_pattern = r\"###------- Step \\d: [\\w\\s]+\"\n",
        "\n",
        "# Split the text based on the pattern\n",
        "split_text = re.split(algorithm_pattern, candidate_prompt)\n",
        "\n",
        "# Extract the algorithm headers (for identification)\n",
        "headers = re.findall(algorithm_pattern, candidate_prompt)\n",
        "\n",
        "# Removing the first empty string from the split if exists (because of the split at the start)\n",
        "split_text = [t.strip() for t in split_text if t.strip()]\n",
        "\n",
        "# Create a dictionary where each algorithm is stored separately\n",
        "algorithms = {headers[i]: split_text[i] for i in range(len(headers))}\n",
        "\n",
        "# Display each algorithm separately\n",
        "steps = []\n",
        "for header, content in algorithms.items():\n",
        "    steps.append( [header, content] )\n",
        "    print(f\"{header}:\\n{content}\\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZCW_Nk7GMKKl",
        "outputId": "d67a8bf8-fd0b-40d1-8efe-856720af032e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "###------- Step 1: Identify the keywords and entities in the question statement:\n",
            ".\n",
            "\n",
            "###------- Step 2: Break down the question into sub:\n",
            "-questions or statements to clarify the context and relationships between the entities.\n",
            "\n",
            "###------- Step 3: Identify any implied information or assumptions that would be necessary to answer the question accurately:\n",
            ".\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for p in range(0, len(prompts)):\n",
        "    # print(candidate_prompt)\n",
        "    # print()\n",
        "\n",
        "    total_right = 0\n",
        "    ADE_total_right = 0\n",
        "    total = 0\n",
        "\n",
        "    for i in range(50, len(data['examples'])):\n",
        "        question = data['examples'][i]['input']\n",
        "        target = data['examples'][i]['target_scores']['Yes'] # if 0 --> No\n",
        "        if target == 0:\n",
        "            target = 'The answer is No' #'No'\n",
        "        else:\n",
        "            target = 'The answer is Yes' #'Yes'\n",
        "\n",
        "\n",
        "        original_prompt = (\n",
        "            \"Given a logic reasoning question, to solve it, you MUST: generate a procedual reasoning from the question statement, and then return the final answer as either 'The answer is Yes' or 'The answer is No'.\" +\n",
        "            f'\\n\\nQuestion: {question}'\n",
        "        )\n",
        "\n",
        "        completion = client.chat.completions.create(\n",
        "            model = model_id, temperature = 0, seed = 0,\n",
        "\n",
        "            messages=[{\"role\": \"system\", \"content\": \"Please solve the logic reasoning question presented by outputting a step-by-step reasoning and then the final answer as either 'The answer is Yes' or 'The answer is No'.\"},\n",
        "                        {\"role\": \"user\", \"content\": original_prompt}],\n",
        "            timeout = 1200)\n",
        "\n",
        "        ori_answer = completion.choices[0].message.content\n",
        "\n",
        "        total += 1\n",
        "        if target in ori_answer:\n",
        "            total_right += 1\n",
        "\n",
        "\n",
        "        overall_with_steps = \"Original question statement: \" + question + '\\n\\n'\n",
        "        for i in range(len(algorithms)):\n",
        "\n",
        "            reformulation_prompt = f'Please thoroughly execute the instruction on the question statement. Instruction:\\n\\n{steps[i][0]}\\n{steps[i][1]}\\n\\n Question statement: {overall_with_steps}.'\n",
        "\n",
        "            completion = client.chat.completions.create(\n",
        "                    model = model_id, temperature = 0.0, max_tokens = 512,\n",
        "\n",
        "                    messages=[{\"role\": \"system\", \"content\": 'Please execute the instruction on the question statement to be informative and detailed for LLM to interpret better. You are allowed to fill in unspecified information based on your domain expertise!'},\n",
        "                                {\"role\": \"user\", \"content\": reformulation_prompt}],\n",
        "                    timeout = 1200)\n",
        "\n",
        "            reformulated_history = completion.choices[0].message.content\n",
        "            # print('Step ' + str(i) + ' Results: ')\n",
        "            # print(reformulated_history)\n",
        "            overall_with_steps += steps[i][0] + '\\n' + steps[i][1] + reformulated_history + '\\n\\n'\n",
        "            # print()\n",
        "            # print()\n",
        "\n",
        "\n",
        "        # print(overall_with_steps)\n",
        "        # print()\n",
        "\n",
        "        ADE_prompt = (\n",
        "            \"Given a logic reasoning question, to solve it, you MUST: generate a procedual reasoning from the question statement, and then return the final answer as either 'The answer is Yes' or 'The answer is No'.\" +\n",
        "            f'\\n\\nQuestion Statment: {overall_with_steps}'\n",
        "        )\n",
        "\n",
        "        completion = client.chat.completions.create(\n",
        "            model = model_id, temperature = 0, seed = 0,\n",
        "\n",
        "            messages=[{\"role\": \"system\", \"content\": \"Please solve the logic reasoning question presented by outputting a step-by-step reasoning and then the final answer as either 'The answer is Yes' or 'The answer is No'.\"},\n",
        "                        {\"role\": \"user\", \"content\": ADE_prompt}],\n",
        "            timeout = 1200)\n",
        "\n",
        "        answer = completion.choices[0].message.content\n",
        "        print(ori_answer)\n",
        "        print(answer)\n",
        "        print(target)\n",
        "\n",
        "\n",
        "        if target in answer:\n",
        "            ADE_total_right += 1\n",
        "\n",
        "        print(f'Total: {total}, Total Right: {total_right}, Total ADE Right: {ADE_total_right}')\n",
        "        print()\n",
        "\n",
        "\n",
        "        if total % 100 == 0:\n",
        "            print(f'Total: {total}, Total Right: {total_right}, Total ADE Right: {ADE_total_right}')\n",
        "            print()\n",
        "\n",
        "            if total_right >= ADE_total_right:\n",
        "                prompt_performances[candidate_prompt] = ADE_total_right / total\n",
        "                print()\n",
        "                break\n",
        "\n",
        "        if total >= 500:\n",
        "            prompt_performances[candidate_prompt] = ADE_total_right / total\n",
        "            break"
      ],
      "metadata": {
        "id": "lravyrwQMKS8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# CoT during tuning:\n",
        "# Total: 168, Total Right: 92, Total ADE Right: 120\n",
        "# Total: 100, Total Right: 51, Total ADE Right: 63"
      ],
      "metadata": {
        "id": "ypmIc9AFPS9i"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}